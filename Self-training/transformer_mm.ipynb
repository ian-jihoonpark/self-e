{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from transformers import GPT2Tokenizer, AutoConfig\n",
    "from PIL import Image\n",
    "from utils import proc_ques\n",
    "from models.gpt import GPT2LMHeadModel\n",
    "import clip_x.clip as clip\n",
    "from transformers import top_k_top_p_filtering\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2, math\n",
    "import matplotlib.pyplot as plt\n",
    "from captum.attr import visualization\n",
    "import json\n",
    "from models.gpt import NLX_GPT\n",
    "from torchvision.transforms import GaussianBlur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_save_path = '/media/storage/coco/caption_data' \n",
    "annFileExp = '../self_training/cococaption/annotations/vqaX_test_annot_exp.json'\n",
    "annFileFull = '../self_training/cococaption/annotations/vqaX_test_annot_full.json'\n",
    "nle_data_test_path = '/media/storage/coco/VQA-X/annotated/vqaX_test.json'\n",
    "nle_data_val_path = '/media/storage/coco/VQA-X/annotated/vqaX_val.json'\n",
    "nle_data_train_path = '/media/storage/coco/VQA-X/annotated/vqaX_train.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")        # load tokenizer\n",
    "# model = GPT2LMHeadModel.from_pretrained(ckpt_path + model_name) # load model with config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_requires_grad(model, req_grad):\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = req_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image resolution: 224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pr03/anaconda3/envs/t5/lib/python3.7/site-packages/torchvision/transforms/transforms.py:333: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
     ]
    }
   ],
   "source": [
    "clip._MODELS = {\n",
    "    \"ViT-B/32\": \"https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt\",\n",
    "    \"ViT-B/16\": \"https://openaipublic.azureedge.net/clip/models/5806e77cd80f8b59890b7e101eabd078d9fb84e6937f9e85e4ecb61988df416f/ViT-B-16.pt\",\n",
    "    \"ViT-L/14\": \"https://openaipublic.azureedge.net/clip/models/b8cca3fd41ae0c99ba7e8951adf17d267cdb84cd88be6f7c2e0eca1737a03836/ViT-L-14.pt\"\n",
    "}\n",
    "\n",
    "clip_model, preprocess = clip.load(\"ViT-B/16\", jit=False)\n",
    "print(\"Image resolution:\", clip_model.visual.input_resolution)\n",
    "image_encoder = clip_model\n",
    "change_requires_grad(image_encoder, False)\n",
    "img_transform = preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2127"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_elements(question_id):\n",
    "    sample = data[question_id]\n",
    "    img_name = sample['image_name']\n",
    "    text_a = proc_ques(sample['question'])    # question\n",
    "\n",
    "    # tokenization process\n",
    "    q_segment_id, a_segment_id, e_segment_id = tokenizer.convert_tokens_to_ids(['<question>', '<answer>', '<explanation>'])\n",
    "    tokens = tokenizer.tokenize(text_a)\n",
    "    segment_ids = [q_segment_id] * len(tokens)\n",
    "\n",
    "    answer = [tokenizer.bos_token] + tokenizer.tokenize(\" the answer is\")\n",
    "    answer_len = len(answer)\n",
    "    tokens += answer \n",
    "\n",
    "    segment_ids += [a_segment_id] * answer_len\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "    segment_ids = torch.tensor(segment_ids, dtype=torch.long)\n",
    "\n",
    "    folder = './media/storage/coco/image/train2014/' if 'train' in img_name else '/media/storage/coco/image/val2014/'   # test and val are both in val2014\n",
    "    img_path = folder + img_name\n",
    "    img = Image.open(img_path)\n",
    "    #.convert('RGB')\n",
    "    img = img_transform(img)\n",
    "    qid = torch.LongTensor([int(question_id)])\n",
    "\n",
    "    return (img, qid, input_ids, segment_ids, img_path)\n",
    "\n",
    "# data list\n",
    "data = json.load(open(nle_data_test_path, \"r\"))\n",
    "ids_list = list(data.keys())\n",
    "len(ids_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/pr03/jh_pjh/xAI/AAAI_2023/Self-training/ckpts/2022-07-07_11:03:06/epoch=05-val_loss=2.15.ckpt were not used when initializing GPT2LMHeadModel: ['epoch', 'global_step', 'hyper_parameters', 'callbacks', 'hparams_name', 'optimizer_states', 'state_dict', 'lr_schedulers', 'pytorch-lightning_version']\n",
      "- This IS expected if you are initializing GPT2LMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2LMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at /home/pr03/jh_pjh/xAI/AAAI_2023/Self-training/ckpts/2022-07-07_11:03:06/epoch=05-val_loss=2.15.ckpt and are newly initialized: ['h.5.mlp.c_fc.bias', 'h.3.crossattention.bias', 'h.0.crossattention.c_attn.weight', 'h.4.crossattention.masked_bias', 'h.4.mlp.c_fc.bias', 'h.1.attn.c_attn.weight', 'h.1.attn.c_proj.bias', 'h.3.crossattention.c_attn.weight', 'h.0.attn.c_proj.weight', 'h.2.ln_2.bias', 'h.5.ln_1.weight', 'h.1.mlp.c_proj.bias', 'h.2.crossattention.q_attn.weight', 'h.5.mlp.c_proj.bias', 'h.0.ln_1.weight', 'h.2.mlp.c_proj.bias', 'h.0.crossattention.c_proj.weight', 'h.0.crossattention.c_proj.bias', 'h.4.mlp.c_fc.weight', 'h.4.ln_2.bias', 'h.0.ln_cross_attn.weight', 'h.1.crossattention.c_proj.weight', 'h.2.mlp.c_fc.bias', 'h.3.mlp.c_fc.bias', 'h.0.mlp.c_proj.weight', 'h.3.crossattention.masked_bias', 'h.5.ln_1.bias', 'h.3.attn.c_attn.weight', 'h.1.crossattention.c_attn.weight', 'h.0.ln_2.bias', 'ln_f.bias', 'h.2.mlp.c_proj.weight', 'h.4.attn.c_proj.bias', 'h.5.crossattention.bias', 'h.3.attn.c_proj.weight', 'h.5.ln_cross_attn.weight', 'h.2.attn.c_attn.weight', 'h.0.ln_2.weight', 'h.1.mlp.c_proj.weight', 'h.1.mlp.c_fc.weight', 'h.2.ln_1.bias', 'h.0.ln_1.bias', 'h.1.crossattention.masked_bias', 'h.4.ln_cross_attn.weight', 'h.1.attn.c_proj.weight', 'h.1.crossattention.c_proj.bias', 'h.4.mlp.c_proj.bias', 'wpe.weight', 'h.0.crossattention.masked_bias', 'h.2.crossattention.masked_bias', 'h.1.ln_1.weight', 'h.4.ln_1.weight', 'h.2.ln_2.weight', 'h.2.attn.c_proj.bias', 'h.1.ln_2.bias', 'h.1.crossattention.q_attn.weight', 'h.5.mlp.c_proj.weight', 'h.4.crossattention.c_proj.bias', 'h.4.attn.c_attn.weight', 'h.4.attn.c_proj.weight', 'h.4.mlp.c_proj.weight', 'h.5.attn.c_proj.bias', 'h.3.ln_2.weight', 'h.0.crossattention.q_attn.weight', 'h.5.crossattention.q_attn.weight', 'h.0.attn.c_proj.bias', 'h.5.crossattention.masked_bias', 'h.5.ln_2.weight', 'h.4.crossattention.c_attn.weight', 'h.3.mlp.c_proj.weight', 'h.2.crossattention.c_attn.weight', 'h.4.ln_1.bias', 'h.3.attn.c_proj.bias', 'h.5.crossattention.c_proj.bias', 'h.3.ln_1.bias', 'h.5.crossattention.c_attn.weight', 'h.1.ln_1.bias', 'h.4.ln_2.weight', 'h.3.mlp.c_fc.weight', 'h.0.mlp.c_fc.bias', 'h.2.ln_cross_attn.weight', 'h.4.crossattention.q_attn.weight', 'h.0.mlp.c_fc.weight', 'h.5.mlp.c_fc.weight', 'h.3.crossattention.q_attn.weight', 'h.4.crossattention.bias', 'h.0.crossattention.bias', 'h.2.mlp.c_fc.weight', 'h.1.ln_cross_attn.weight', 'h.3.mlp.c_proj.bias', 'h.2.ln_1.weight', 'h.4.crossattention.c_proj.weight', 'h.5.ln_2.bias', 'h.5.crossattention.c_proj.weight', 'h.2.attn.c_proj.weight', 'h.3.ln_2.bias', 'h.3.ln_cross_attn.weight', 'ln_f.weight', 'h.0.attn.c_attn.weight', 'h.0.mlp.c_proj.bias', 'h.1.mlp.c_fc.bias', 'wte.weight', 'h.2.crossattention.c_proj.weight', 'h.1.crossattention.bias', 'h.5.attn.c_attn.weight', 'h.5.attn.c_proj.weight', 'h.3.ln_1.weight', 'h.3.crossattention.c_proj.bias', 'h.2.crossattention.c_proj.bias', 'h.3.crossattention.c_proj.weight', 'h.1.ln_2.weight', 'h.2.crossattention.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained('distilgpt2')\n",
    "config.add_cross_attention=True\n",
    "model = GPT2LMHeadModel.from_pretrained(\"/home/pr03/jh_pjh/xAI/AAAI_2023/Self-training/ckpts/2022-07-07_11:03:06/epoch=05-val_loss=2.15.ckpt\", config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24757/4138628257.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mnlx_gpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNLX_GPT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvisual_backbone\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlm_backbone\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2, math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from models.gpt import NLX_GPT\n",
    "\n",
    "\n",
    "load_from_epoch = 11\n",
    "ckpt_path = '../self_training/ckpts/VQAX_p/'\n",
    "\n",
    "SPECIAL_TOKENS = ['<|endoftext|>', '<pad>', '<question>', '<answer>', '<explanation>']\n",
    "special_tokens_ids = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS)\n",
    "because_token_id = tokenizer.convert_tokens_to_ids('Ä because')\n",
    "\n",
    "\n",
    "nlx_gpt = NLX_GPT(visual_backbone=image_encoder, lm_backbone=model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image_relevance(image_relevance, image, orig_image):\n",
    "    # create heatmap from mask on image\n",
    "    def show_cam_on_image(img, mask):\n",
    "        heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
    "        heatmap = np.float32(heatmap) / 255\n",
    "        cam = heatmap + np.float32(img)\n",
    "        cam = cam / np.max(cam)\n",
    "        return cam\n",
    "\n",
    "    # plt.axis('off')\n",
    "    # f, axarr = plt.subplots(1,2)\n",
    "    # axarr[0].imshow(orig_image)\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2)\n",
    "    axs[0].imshow(orig_image);\n",
    "    axs[0].axis('off');\n",
    "    \n",
    "    feat_hw = int(math.sqrt(image_relevance.shape[-1]))\n",
    "    image_relevance = image_relevance.reshape(1, 1, feat_hw, feat_hw)\n",
    "    image_relevance = torch.nn.functional.interpolate(image_relevance, size=224, mode='bilinear')\n",
    "    image_relevance = image_relevance.reshape(224, 224).cuda().data.cpu().numpy()\n",
    "    image_relevance = (image_relevance - image_relevance.min()) / (image_relevance.max() - image_relevance.min())\n",
    "    image = image[0].permute(1, 2, 0).data.cpu().numpy()\n",
    "    image = (image - image.min()) / (image.max() - image.min())\n",
    "    vis = show_cam_on_image(image, image_relevance)\n",
    "    vis = np.uint8(255 * vis)\n",
    "    vis = cv2.cvtColor(np.array(vis), cv2.COLOR_RGB2BGR)\n",
    "    # axar[1].imshow(vis)\n",
    "    axs[1].imshow(vis);\n",
    "    axs[1].axis('off');\n",
    "    # plt.imshow(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 224\n",
    "max_seq_len = 40\n",
    "do_sample = False\n",
    "top_k =  0\n",
    "top_p =  0.9\n",
    "temperature = 1\n",
    "start_layer = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_id = ids_list[0]\n",
    "batch = get_elements(q_id)\n",
    "img_path = batch[-1]\n",
    "batch = tuple(input_tensor.unsqueeze(0).to(device) for input_tensor in batch[:-1])\n",
    "img, img_id, input_ids, segment_ids = batch\n",
    "batch_size = img.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24757/1717942975.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'batch' is not defined"
     ]
    }
   ],
   "source": [
    "batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_relevance_maps = []\n",
    "current_output = []\n",
    "current_logits = []\n",
    "always_exp = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12, 7, 7])\n",
      "torch.Size([1, 12, 7, 196])\n",
      "torch.Size([1, 12, 7, 7])\n",
      "torch.Size([1, 12, 7, 196])\n",
      "torch.Size([1, 12, 7, 7])\n",
      "torch.Size([1, 12, 7, 196])\n",
      "torch.Size([1, 12, 7, 7])\n",
      "torch.Size([1, 12, 7, 196])\n",
      "torch.Size([1, 12, 7, 7])\n",
      "torch.Size([1, 12, 7, 196])\n",
      "torch.Size([1, 12, 7, 7])\n",
      "torch.Size([1, 12, 7, 196])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50257])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = nlx_gpt(image=img, input_ids=input_ids, segment_ids=segment_ids)\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_attn_blocks = list(dict(nlx_gpt.visual_encoder.visual.transformer.resblocks.named_children()).values())\n",
    "num_tokens = image_attn_blocks[0].attn_probs.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50257])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_32824/539655250.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprobas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprobas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "probas = logits.softmax(-1)[0, :, :-1]\n",
    "probas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50257])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import top_k_top_p_filtering\n",
    "filtered_logits = top_k_top_p_filtering(logits, top_k=top_k, top_p=top_p)\n",
    "filtered_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50257])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = F.softmax(filtered_logits, dim=-1)\n",
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[24829, 13965, 10130, 21311,  6787, 20478, 27518]], device='cuda:0')"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev2 = torch.multinomial(probs,7)\n",
    "prev2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[20457]], device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev = torch.multinomial(probs, dim=-1) if do_sample else torch.argmax(filtered_logits, dim=-1).unsqueeze(-1)\n",
    "prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot = F.one_hot(prev, num_classes=logits.shape[-1]).type(torch.float32)\n",
    "one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = one_hot.requires_grad_(True).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.3458]], device='cuda:0', grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot = torch.sum(one_hot.cuda()*logits, dim=-1)\n",
    "one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlx_gpt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 197, 197])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad = torch.autograd.grad(one_hot, [image_attn_blocks[11].attn_probs], retain_graph=True)[0].detach()\n",
    "grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, blk in enumerate(image_attn_blocks):\n",
    "        if i < start_layer:\n",
    "            continue\n",
    "        grad = torch.autograd.grad(one_hot, [blk.attn_probs], retain_graph=True)[0].detach()\n",
    "        cam = blk.attn_probs.detach()\n",
    "        cam = cam.reshape(-1, cam.shape[-1], cam.shape[-1])\n",
    "        grad = grad.reshape(-1, grad.shape[-1], grad.shape[-1])\n",
    "        cam = grad * cam\n",
    "        cam = cam.reshape(batch_size, -1, cam.shape[-1], cam.shape[-1])\n",
    "        cam = cam.clamp(min=0).mean(dim=1)\n",
    "        R = R + torch.bmm(cam, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_blocks = list(dict(nlx_gpt.visual_encoder.visual.transformer.resblocks.named_children()).values())\n",
    "decoder_blocks = nlx_gpt.lm.transformer.h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_bboxes = encoder_blocks[0].attn_probs.shape[-1]\n",
    "queries_num = decoder_blocks[0].attn.get_attn().shape[-1]\n",
    "\n",
    "# image self attention matrix\n",
    "R_i_i = torch.eye(image_bboxes, image_bboxes).to(encoder_blocks[0].attn_probs.device)\n",
    "# queries self attention matrix\n",
    "R_q_q = torch.eye(queries_num, queries_num).to(encoder_blocks[0].attn_probs.device)\n",
    "# impact of image boxes on queries\n",
    "R_q_i = torch.zeros(queries_num, image_bboxes-1).to(encoder_blocks[0].attn_probs.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 197, 197])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_blocks[0].attn_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_heads(cam, grad):\n",
    "    cam = cam.reshape(-1, cam.shape[-2], cam.shape[-1])\n",
    "    grad = grad.reshape(-1, grad.shape[-2], grad.shape[-1])\n",
    "    cam = grad * cam\n",
    "    cam = cam.clamp(min=0).mean(dim=0)\n",
    "    return cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "for blk in encoder_blocks:\n",
    "    grad = torch.autograd.grad(one_hot, [blk.attn_probs], retain_graph=True)[0].detach()\n",
    "    cam = blk.attn_probs.detach()\n",
    "    cam = avg_heads(cam, grad)\n",
    "    R_i_i += torch.matmul(cam, R_i_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_self_attention_rules(R_ss, R_sq, cam_ss):\n",
    "    R_sq_addition = torch.matmul(cam_ss, R_sq)\n",
    "    R_ss_addition = torch.matmul(cam_ss, R_ss)\n",
    "    return R_ss_addition, R_sq_addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "for blk in decoder_blocks:\n",
    "    grad = torch.autograd.grad(one_hot, [blk.attn.get_attn()], retain_graph=True)[0].detach()\n",
    "    cam = blk.attn.get_attn().detach()\n",
    "    cam = avg_heads(cam, grad)\n",
    "    R_q_q_add, R_q_i_add = apply_self_attention_rules(R_q_q, R_q_i, cam)\n",
    "    R_q_q += R_q_q_add\n",
    "    R_q_i += R_q_i_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_residual(orig_self_attention):\n",
    "    self_attention = orig_self_attention.clone()\n",
    "    diag_idx = range(self_attention.shape[-1])\n",
    "    self_attention -= torch.eye(self_attention.shape[-1]).to(self_attention.device)\n",
    "    assert self_attention[diag_idx, diag_idx].min() >= 0\n",
    "    self_attention = self_attention / self_attention.sum(dim=-1, keepdim=True)\n",
    "    self_attention += torch.eye(self_attention.shape[-1]).to(self_attention.device)\n",
    "    return self_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mm_attention_rules(R_ss, R_qq, cam_sq, apply_normalization=True, apply_self_in_rule_10=True):\n",
    "    R_ss_normalized = R_ss\n",
    "    R_qq_normalized = R_qq\n",
    "    if apply_normalization:\n",
    "        R_ss_normalized = handle_residual(R_ss)\n",
    "        R_qq_normalized = handle_residual(R_qq)\n",
    "    R_sq_addition = torch.matmul(R_ss_normalized.t(), torch.matmul(cam_sq, R_qq_normalized))\n",
    "    if not apply_self_in_rule_10:\n",
    "        R_sq_addition = cam_sq\n",
    "    R_sq_addition[torch.isnan(R_sq_addition)] = 0\n",
    "    \n",
    "    return R_sq_addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "for blk in decoder_blocks:\n",
    "    grad_q_i = torch.autograd.grad(one_hot, [blk.crossattention.get_attn()], retain_graph=True)[0].detach()\n",
    "    cam_q_i = blk.crossattention.get_attn().detach()\n",
    "    cam_q_i = avg_heads(cam_q_i, grad_q_i)\n",
    "    R_q_i += apply_mm_attention_rules(R_q_q, R_i_i[1:,1:], cam_q_i,apply_normalization=True,apply_self_in_rule_10=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 196])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated = R_q_i.unsqueeze_(0)\n",
    "aggregated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated = aggregated[:,target_index, :].unsqueeze_(0).detach()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('t5')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "920bff935c1399c8ca1f22b0319a3cb298b983f35fbad2f58be88cbfac3be712"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
